---
title: "Patient Survival Prediction Project"
author: "Adam Crawford, Adam Baer, Austin Funcheon, Viraj Rane"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Data

```{r}
df <- read.csv("dataset.csv")

#str(df)
#summary(df)

library("dplyr")
library("ggpubr")
library(tree)
library(caret)
library(randomForest)
library(lattice)
library(dplyr)


library(ggplot2)
#ggplot(df, aes(factor(hospital_death))) +
#  geom_bar()
```

# Data Preparation


```{r}
# get rid of identifiers and random X column
df$encounter_id <- df$patient_id <- df$hospital_id <- df$X <- NULL

# change columns to factors
factors <- c("elective_surgery","ethnicity","gender","icu_admit_source",
             "icu_stay_type","icu_type","apache_post_operative","arf_apache",
             "gcs_eyes_apache","gcs_motor_apache","gcs_unable_apache","gcs_verbal_apache",
             "intubated_apache","ventilated_apache","aids","cirrhosis",
             "diabetes_mellitus","hepatic_failure","immunosuppression","leukemia",
             "lymphoma","solid_tumor_with_metastasis","apache_3j_bodysystem",
             "hospital_death")
df[factors] <- lapply(df[factors], factor)


df0 <- df # keeping an origin copy of df. I suspect we don't want to blanket kill all the omits if we later find those factors are irrelevant. our dataset cuts in half ~Austin

# Adam - what do we want to do with NA's?
df <- na.omit(df)
```

```{r}
# split data into training data and testing data
set.seed(123)
train <- sample(1:nrow(df), nrow(df)*0.7)
train_data <- df[train,]
test_data <- df[-train,]
```

```{r}
library(tree)

#this doesn't work
first_tree <- tree(as.factor(hospital_death) ~., data = train_data)
summary(first_tree)
plot(first_tree)
text(first_tree, cex = 0.75, col = 'red')
```


Import data	                                                          Crawford
Store data into a dataframe	                                          Crawford
Perform high level review of data	                                    Crawford
Run prelim statistics on data	                                        Crawford
Review data for missing data	                                        Baer
Identify High integrity predictors                                    Baer
Remove unhelpful predictors (Such as, perhaps, ID)                    Baer
Transform data for usability	                                        Rane
Visualize data	                                                      Baer
<<<<<<< HEAD


Identify if any continuous data is non-normal	                        Funcheon
```{r}
#install.packages("dplyr")
#library("dplyr")
#library("ggpubr")
#is it a character?
#ischar <- sapply(df, is.character)
#ischar
#df1 <- df
#visualize data
```




=======
#>>>>>>> 62c6a16839bd4464e86e1644872e6cb213b68af8
Identify if any discrete data is heavily unbalanced                   Rane

Cleanse data                                                        	Funcheon
```{r}
#Austin
#summary(df)
#dfF <- df

#dfF %>%
#  filter(apache_4a_hospital_death_prob >= 0)

#print("Filter1")
#summary(dfF)
#str(df)

```

```{r}
#Austin
# Fit a random forest for variable selection, run time ~7.5 minutes
library(randomForest)
#set.seed(123)
start_t <- Sys.time()
cat("",cat(" Variable selection Training started at:",format(start_t, "%a %b %d %X %Y")))

#uncomment this to run again.
rf_eval <- randomForest(hospital_death~., data = train_data,
                           mtry = 5, importance = TRUE)

rf_eval
varImpPlot(rf_eval)

finish_t <- Sys.time()
cat("",cat("Variable selection Training finished at:",format(finish_t, "%a %b %d %X %Y")))

cat("Variable selection The training process finished in",difftime(finish_t,start_t,units="mins"), "minutes")


```

```{r} 
#Austin Identify the lowest impact variables from the RF run
library(lattice)
library(dplyr)
rfImp <- importance(rf_eval)
#rfImp
# Export the importance list to csv ~Austin
#write.csv(rfImp, "varSelection.csv")

rf_Imp_Sort <- as.data.frame.matrix(rfImp)
#adds a column for accuracy*gini for overall variable impact
rf_Imp_Sort$cross <- rf_Imp_Sort$MeanDecreaseAccuracy*rf_Imp_Sort$MeanDecreaseGini
#rf_Imp_Sort
#adds an overall rank of impact of variable, with bigger being biggest ranked impact, with 1 being lowest impact
rf_Imp_Sort$rank <- rank(rf_Imp_Sort$cross)
#rf_Imp_Sort

#variables ranked by cross order
rf_Imp_SO <- rf_Imp_Sort[order(-rf_Imp_Sort$cross),]
#rf_Imp_SO
#variable list sorted by importance. Accuracy * Gini
```

```{r} 
#Check accuracy score
rf_yhat <- predict(rf_eval, newdata = test_data)
#accuracy score of mtry 5 rf used for variable selection
rfscore <- postResample(rf_yhat, test_data$hospital_death)
print(rfscore)
```

```{r}
rfVarDrop <-rf_Imp_Sort
#trim the bottom 30 predictors
rfVarDrop <- rfVarDrop %>% filter(rank <= 30)
#rfVarSel

dropVar <- row.names(rfVarDrop)
#keepVar

#make a shorter list of variables from rf variable selection
dfTrim <- df0[ , ! names(df0) %in% dropVar] 
dfTrim <- na.omit(dfTrim)
#dfTrim is experemental list.

```

```{r}
#create new train and test data from rf selection
train2 <- sample(1:nrow(dfTrim), nrow(dfTrim)*0.7)
trim_train_data <- dfTrim[train2,]
trim_test_data <- dfTrim[-train2,]
#str(dfTrim)
```

```{r}
start_t <- Sys.time()
cat("",cat("Trimmed Training started at:",format(start_t, "%a %b %d %X %Y")))


rf_eval2 <- randomForest(hospital_death~., data = trim_train_data,
                           mtry = 5, importance = TRUE)

rf_eval2
varImpPlot(rf_eval2)

finish_t <- Sys.time()
cat("",cat("Trimmed Training finished at:",format(finish_t, "%a %b %d %X %Y")))

cat("Trimmed The training process finished in",difftime(finish_t,start_t,units="mins"), "minutes")
```

```{r} 
#Check accuracy score
rf_yhat2 <- predict(rf_eval2, newdata = trim_test_data)
#accuracy score of mtry 5 rf used for variable selection
rfscore2 <- postResample(rf_yhat2, trim_test_data$hospital_death)
rfscore2
rfscore
```



Consider data normalization	                                          Rane
```{r}
#library(caret)

```

Consider balancing data	                                              Funcheon
Consider transformation of some fields, such as log transformation	  Funcheon
```{r}
#ggqqplot(df$h1_resprate_max)
#df1 <- df
#df1$h1_resprate_max <- log(df$h1_resprate_max)
#ggqqplot(df1$h1_resprate_max)
```

Format datafields into data levels (if needed)                    	  Funcheon
Run statistical analysis of predictors  .                           	Rane
Prune unhelpful predictors to drive to parsimonous model.	            Rane
Evaluate for collinearity	                                            Baer
Reduce model for predictors that have high colinearity	              Baer
Re-run statistical analysis on reduced model	                        Crawford
Break data up into train test and validate data set	                  Crawford
Run model type #1	                                                    Crawford
Evaluate results of model type #1	                                    Crawford
Run model type #2	                                                    Crawford
Evaluate results of model type #2	                                    Crawford
Run model type #3	                                                    Rane
Evaluate results of model type #3	                                    Rane
Run model type #4	                                                    Funcheon
Evaluate results of model type #4	                                    Funcheon
Tune parameters of favored model                                    	Rane
Validate favored model on Test group                                	Rane

